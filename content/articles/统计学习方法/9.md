Title: 统计学习方法 第九章 EM 算法及其推广
Category: 读书笔记
Date: 2018-11-16 20:59:28
Modified: 2018-11-16 20:59:28
Tags: 统计学习, 机器学习

EM 算法是一种迭代算法，用于含有隐变量（hidden varibale）的概率模型参数的极大似然估计，或极大后验概率估计。EM 算法每次的迭代分两步：E 步，求期望（expectation）；M 步，求加大（maximization）。所以这一算法被称为期望极大算法（expectation maximization）。

## 9.1 EM 算法的引入

概率模型有时既含有观测变量（observable variable），又含有隐变量或潜在变量（latent variable）。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单的使用这些估计方法。EM 算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。

一般的，用 $Y$ 表示观测随机变量的数据，$Z$ 表示隐随机变量的数据。$Y$ 和 $Z$ 连在一起称为完全数据（complete-data），观测数据 $Y$ 又称为不完全数据（incomplete-data）。假设给定观测数据 $Y$，其概率分布是 $P(Y|\theta)$，其中 $\Theta$ 是需要估计的模型参数，那么不完全数据了 $Y$ 的似然函数是 $P(Y|\theta)$，对数似然函数是 $L(\theta)=\log P(Y|\theta)$；假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y,Z|\theta)$，那么完全数据的对数似然函数是 $\log P(Y,Z|\theta)$。

EM 算法通过迭代求 $L(\theta)=\log P(Y|\theta)$ 的极大似然估计。

**
算法 9.1（EM 算法）  
输入：观测变量数据 $Y$，隐变量数据 $Z$，联合分布 $P(Y,Z|\theta)$，条件分布 $P(Z|Y,\theta)$  
输出：模型参数 $\theta$  
(1) 选择参数的初值 $\theta^{(0)}$，开始迭代  
(2) E 步：记 $\theta^{(i)}$ 为第 $i$ 次迭代参数 $\theta$ 的估计值。在第 $i+1$ 次迭代的 E 步，计算
$$\begin{eqnarray}
Q\left(\theta,\theta^{(i)}\right) &=& \text{E}_Z\left[\log P(Y,Z|\theta)|Y,\theta^{(i)}\right] \\
&=& \sum_{Z}\log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
\end{eqnarray}$$
这里，$P(Z|Y,\theta^{(i)})$ 是在给定观测数据 $Y$ 和当前的参数估计 $\theta^{(i)}$ 下隐变量数据 $Z$ 的条件概率分布  
(3) M 步：求极大，更新 $\theta$  
$$\theta^{(i+1)}=\arg\max_{\theta}Q\left(\theta,\theta^{(i)}\right)$$
(4) 重复 (2)，(3)，直到收敛
**

$Q$ 函数是 EM 算法的核心。

**
定义 9.1（$Q$ 函数）玩去数据的对数似然函数 $\log P(Y,Z|\theta)$ 关于在给定观测数据 $Y$ 的当前参数 $\theta^{(i)}$ 下对未观测数据 $Z$ 的条件概率分布 $P\left(Z|Y,\theta^{(i)}\right)$ 的期望称为 $Q$ 函数，即
$$Q\left(\theta,\theta^{(i)}\right)=\text{E}_Z\left[\log P(Y,Z|\theta)|Y,\theta^{(i)}\right]$$
**

关于 EM 算法需要注意：参数的初值可以任意选择，但需注意 EM 算法对初值是敏感的；给出停止迭代的条件，一般是对较小的正数 $\epsilon_1$，$\epsilon_2$，若满足
$$\|\theta^{(i+1)}-\theta^{(i)}\|<\epsilon_1 $$
或
$$\left\|Q\left(\theta^{(i+1)},\theta^{(i)}\right)-Q\left(\theta^{(i)},\theta^{(i)}\right)\right\|<\epsilon_2$$
则迭代停止。
