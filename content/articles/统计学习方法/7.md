Title: 统计学习方法 第七章 支持向量机
Category: 读书笔记
Date: 2018-11-11 13:47:48
Modified: 2018-11-11 17:55:33
Tags: 统计学习, 机器学习

支持向量机（support vector machines，SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。

支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。

支持向量机学习方法包含构建由间至繁的模型：

- 线性可分支持向量机（linear support vector machine in linearly separable case）：数据线性可分，通过硬间隔最大化（hard margin maximization），学习一个线性分类器
- 线性支持向量机（linear support vector machine）：数据近似线性可分，通过软间隔最大化（soft margin maximization），学习一个线性分类器，又称软间隔支持向量机
- 非线性支持向量机（non-linear support vector machine）：数据线性不可分，通过核技巧（kernel trick）及软间隔最大化，学习一个非线性分类器

## 7.1 线性可分支持向量机与硬间隔最大化

### 7.1.1 线性可分支持向量机

一般的，当训练数据集线性可分时，存在无穷多个分离超平面可将两类数据正确分开。感知机利用分类误差最小的策略，求得分离超平面，不过这是的解有无穷多个。线性可分支持向量机利用间隔最大化求解最优分离超平面，这时，解是唯一的。

**
定义 7.1（线性可分支持向量机）给定线性可分训练数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为
$$w^\star\cdot x+b^\star=0$$
以及相应的分类决策函数
$$f(x)=\text{sign}(w^\star\cdot x+b^\star)$$
称为线性可分支持向量机。
**

### 7.1.2 函数间隔和几何间隔

**
定义 7.2（函数间隔）对于给定的训练数据集 $T$ 和超平面 $(w,b)$，定义超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为
$$\hat{\gamma}_i=y_i(w\cdot x_i+b)$$
定义超平面 $(w,b)$ 关于训练数据集 $T$ 的函数间隔为
$$\hat{\gamma}=\min_{i=1,2,\cdots,N}\hat{\gamma}_i$$
**

函数间隔（functional margin）可以表示分类预测的正确性和确信度，但是具有不确定因子，我们需要对分离超平面的法向量 $w$ 加以约束，使得间隔是确定的。这时函数间隔就成了几何间隔（geometric margin）。

**
定义 7.2（几何间隔）对于给定的训练数据集 $T$ 和超平面 $(w,b)$，定义超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔为
$$\gamma_i=y_i\left(\frac{w}{\|w\|}\cdot x_i+\frac{b}{\|w\|}\right)$$
定义超平面 $(w,b)$ 关于训练数据集 $T$ 的几何间隔为
$$\gamma=\min_{i=1,2,\cdots,N}\gamma_i$$
**

易知：
$$\gamma_i=\frac{\hat{\gamma}_i}{\|w\|} \\
\gamma=\frac{\hat{\gamma}}{\|w\|}$$
如果超平面参数 $w$ 和 $b$ 成比例变化（超平面没有改变），函数间隔也按此比例变化，几何间隔不变。

### 7.1.3 间隔最大化

间隔最大化的直观解释：对训练数据集找到几何间隔最大的超平面意味着以充分大的确定度对训练数据进行分类。这样的超平面对未知的新实例有很好的分类预测能力。

#### 最大间隔分离超平面

求解几何间隔最大的分离超平面问题即求解下面的约束最优化问题：
$$\max_{w,b}\ \gamma \\
\text{s.t.}\ \ \ \ y_i\left(\frac{w}{\|w\|}\cdot x_i+\frac{b}{\|w\|}\right)\geq\gamma,\ i=1,2,\cdots,N$$
可改写为：
$$\max_{w,b}\ \frac{\hat{\gamma}}{\|w\|} \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)\geq\hat{\gamma},\ i=1,2,\cdots,N$$
对于上式来说，假设将分离超平面 $(w,b)$ 按比例改变为 $(\lambda w,\lambda b)$，这时函数间隔变为 $\lambda \hat{\gamma}$，所以对目标函数和约束条件都没有影响。那么，如果我们做如下变换：
$$(w,b)\to(\frac{1}{\hat{\gamma}}w,\frac{1}{\hat{\gamma}}b)$$
则函数间隔变为 1，故上述问题可改写为：
$$\max_{w,b}\ \frac{1}{\|w\|} \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)\geq 1,\ i=1,2,\cdots,N$$
注意到最大化 $\frac{1}{\|w\|}$ 和最小化 $\frac{1}{2}\|w\|^2$ 是等价的，于是问题变为：
$$\min_{w,b}\ \frac{1}{2}\|w\|^2 \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)-1\geq0,\ i=1,2,\cdots,N$$
这是一个凸二次规划问题。

凸优化问题是指约束最优化问题
$$\min_w\ f(w) \\
\text{s.t.}\ \ \ \ \begin{eqnarray}
g_i(x) &\leq0&,\ i=1,2\cdots,k \\
h_i(w) &=& 0,\ i=1,2,\cdots,l
\end{eqnarray}
$$
其中，目标函数 $f(w)$ 和约束函数 $g_i(w)$ 都是 $\mathbb{R}^n$ 上的连续可微凸函数，约束函数 $h_i(w)$ 是 $\mathbb{R}^n$ 上的仿射函数（即满足 $h_i(w)=a\cdot w+b$ 的形式）。

**
算法 7.1（线性可分支持向量机学习算法——最大间隔法）  
输入：线性可分训练数据集 $T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中 $x_i\in{\cal X}=\mathbb{R}^n$，$y_i\in{\cal Y}=\{+1,-1\}$，$i=1,2,\cdots,N$  
输出：最大间隔分离超平面和分类决策函数  
(1) 构造并求解约束最优化问题
$$\min_{w,b}\ \frac{1}{2}\|w\|^2 \\
\text{s.t.}\ \ \ \ y_i\left(w\cdot x_i+b\right)-1\geq0,\ i=1,2,\cdots,N$$
求得最优解 $w^\star$，$b^\star$  
(2) 由此得到分离超平面
$$w^\star\cdot x+b^\star=0$$
分类决策函数
$$f(x)=\text{sign}(w^\star\cdot x+b^\star)$$
**

上面的算法就是最大间隔法（maximum margin method）。

#### 最大间隔分离超平面的存在唯一性

**
定理 7.1（最大间隔分离超平面的存在唯一性）若训练数据集线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。
**
