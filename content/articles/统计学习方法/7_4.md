Title: 统计学习方法 第七章 支持向量机（4）——序列最小最优化算法
Category: 读书笔记
Tags: 统计学习, 机器学习

## 7.4 序列最小最优化算法

序列最小优化算法（Sequential minimal optimization, SMO）是一种用于解决支持向量机训练过程中所产生优化问题的算法。SMO 由微软研究院的约翰·普莱特于 1998 年发明，目前被广泛使用于 SVM 的训练过程中，并在通行的 SVM 库 LIBSVM 中得到实现。1998 年，SMO 算法发表在 SVM 研究领域内引起了轰动，因为先前可用的 SVM 训练方法必须使用复杂的方法，并需要昂贵的第三方二次规划工具。而 SMO 算法较好地避免了这一问题。

SMO 算法主要用于解决如下凸二次规划的对偶问题
$$\min_{\alpha} \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i \\
\text{s.t.}\ \ \ \ \begin{eqnarray}
\sum_{i=1}^N\alpha_iy_i &=& 0 \\
0\leq\alpha_i\leq C,\ i &=& 1,2,\cdots,N
\end{eqnarray}$$
在这个问题中，变量是拉格朗日乘子，一个变量 $\alpha_i$ 对应一个样本点 $(x_i,y_i)$，变量的总数等于训练样本容量 $N$。


SMO 算法是一种启发式算法，基本思路是：如果所有变量的解都满足此最优化问题的 KKT 条件，那么这么最优化问题的解就得到了。因为 KKT 条件是该最优化问题的充要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，这个二次规划问题的关于这两个变量的解应该更接近原始二次规划问题的解，重要的是，这两个变量可以通过解析方法来求解，这样就可以大大提高整个算法的计算速度。

整个 SMO 算法有两大部分组成，第一部分就是选择这两个变量的启发式的方法，第二部分是求解这两个变量的解析方法。

### 7.4.1 两个变量二次规划的求解方法

不失一般性，假设选择的两个变量是 $\alpha_1$，$\alpha_2$，其他变量 $\alpha_i\ (i=3,4,\cdots,N)$ 是固定的。于是 SMO 的最优化问题的子问题可以写成：
$$\min_{\alpha_1,\alpha_2}\ \ \begin{eqnarray}
W(\alpha_1,\alpha_2) &=& \frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2 \\
&-& (\alpha_1+\alpha_2)+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2}
\end{eqnarray}$$
$$\text{s.t.}\ \ \ \ \begin{eqnarray}
\alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^N\alpha_iy_i &=& \varsigma \\
0\leq\alpha_i\leq C,\ i = 1,2,\cdots,&N&
\end{eqnarray}$$
其中，$K_{ij}=K(x_i,x_j)$，$\varsigma$ 是常数，并忽略了不含 $\alpha_{1}$，$\alpha_2$ 的常数项。

![二变量优化问题]({filename}/images/statistical_learning_7.4.png)

上图中显示了 $\alpha_{1}$，$\alpha_2$ 的取值范围，即位于平行于对角线的线段之上。所以两变量最优化问题实际上为单变量最优化问题，不妨考虑变量 $\alpha_2$ 的最优化问题。

假设二次规划问题的初始可行解是 $\alpha_1^\text{old}$，$\alpha_2^\text{old}$，最优解为 $\alpha_1^\text{new}$，$\alpha_2^\text{new}$，并且假设在沿着约束方向未经剪辑时 $\alpha_2$ 的最优解为 $\alpha_2^\text{new,unc}$。
