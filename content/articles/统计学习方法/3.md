Title: 统计学习方法 第三章 k 近邻法
Category: 读书笔记
Date: 2018-11-04 23:11:36
Modified: 2018-11-05 17:57:15
Tags: 统计学习, 机器学习

$k$ 近邻法（$k$-nearest neighbor，$k$-NN）是一种基本分类与回归方法。$k$ 近邻法的输入为实例的特征向量，输出为实例的类别，可以取多类。$k$ 近邻法假设给定一个训练集数据，其中的实例类别已定。分类时，对新的实例，根据其 $k$ 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，$k$ 近邻法不具有显式的学习过程。

$k$ 近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的模型。

$k$ 值得选择、距离度量以及分类决策规则是 $k$ 近邻法的三个基本要素。

## 3.1 $k$ 近邻算法

**算法 3.1 （$k$ 近邻法）  
输入：训练数据集
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
其中
$$x_i\in{\cal X}\subseteq\mathbb{R}^n$$
为实例的特征空间，
$$y_i\in{\cal Y}=\{c_1,c_2,\cdots,c_K\}$$
为实例的类别；
实例特征向量 $x$  
输出：实例 $x$ 所属的类 $y$  
(1) 根据给定的距离度量，在训练集 $T$ 中找出与 $x$ 最邻近的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_k(x)$  
(2) 在 $N_k(x)$  中根据分类决策规则（如多数表决）决定 $x$ 的类别 $y$
$$y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}\mathbb{I}(y_i=c_j),i=1,2,\cdots,N; j=1,2,\cdots,K$$
式中，$\mathbb{I}$ 为指示函数**

$k$ 近邻法的特殊情况是 $k=1$ 的情形，称为最近邻算法。对于输入的实例点（特征向量）$x$，最邻近法将数据集中与 $x$ 最邻近点的类作为 $x$ 的类。

## 3.2 $k$ 近邻模型

$k$ 近邻法使用的模型实际上对应于特征空间的划分。模型由三个基本要素——距离度量，$k$ 值得选取和分类决策规则决定。

### 3.2.1 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反应。$k$ 近邻模型的特征空间一般是 $n$ 维实数向量空间 $\mathbb{R}^n$。使用的距离是欧氏距离，也可以使用其他距离，例如更一般的 $L_p$ 距离或 Minkowski 距离。

设特征空间 ${\cal X}$ 是 $n$ 维实数向量空间 $\mathbb{R}^n$，$x_i,x_j\in{\cal X}$，$x_i=\left(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)}\right)^\text{T}$，$x_j=\left(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)}\right)^\text{T}$，$x_i,x_j$ 的 $L_p$ 距离定义为
$$L_p(x_i,x_j)=\left(\sum_{l=1}^n\left|x_i^{(l)}-x_j^{(l)}\right|^p\right)^\frac{1}{p}$$

### 3.2.2 $k$ 值的选择

$k$ 值得选择会对 $k$ 近邻法的结果产生重大影响。

如果选择较小的 $k$ 值，就相当于用较小的邻域中的训练实例进行预测。“学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声点，预测就会出错。换句话说，$k$ 值得减小就意味着整体模型变的复杂，容易发生过拟合。

如果 $k$ 值选择过大，就相当于用较大邻域中的训练实例进行预测。其优点是可以减小学习的估计误差，但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。$k$ 值的增大意味着整体的模型变得简单。

如果 $k=N$，那么无论输入的实例是什么，都将简单的预测它属于在训练实例中最多的类，这时，模型过于简单，是不可取的。

在应用中，$k$ 值一般取一个比较小的数值，通常采用交叉验证法来选取最优的 $k$ 值。

### 3.2.3 分类决策规则

$k$ 近邻法中的分类决策规则往往是多数表决，即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。

多数表决规则（majority voting rule）有如下解释：如果分类的损失函数是 0-1 损失函数，分类函数为
$$f:\mathbb{R}^n\longrightarrow\{c_1,c_2,\cdots,c_K\}$$
那么误分类的概率是
$$P(Y\neq f(X))=1-P(Y=f(X))$$
对于给定的实例 $x\in{\cal X}$，其最邻近的 $k$ 个训练实例点构成集合 $N_k(x)$。如果涵盖 $N_k(x)$ 的区域类别是 $c_j$，那么误分类率是
$$\frac{1}{k}\sum_{x_i\in N_k(x)}\mathbb{I}(y_i\neq c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}\mathbb{I}(y_i= c_j)$$
要使误分类率最小即经验风险最小，就要使 $\sum_{x_i\in N_k(x)}\mathbb{I}(y_i= c_j)$ 最大，所以多数表决规则等价于经验风险最小化。
