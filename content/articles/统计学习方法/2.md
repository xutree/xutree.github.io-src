Title: 统计学习方法 第二章 感知机
Category: 读书笔记
Date: 2018-11-04 21:04:02
Modified: 2018-11-04 21:19:51
Tags: 统计学习, 机器学习

感知机（perceptron）是二分类的线性分类模型，其输入是实例的特征向量，输出为实例的类别，取 +1 和 -1 二值。感知机对应于输入空间（特征空间）中将实例划分为正负类别的分离超平面，属于判别模型。

## 2.1 感知机模型

**定义 2.1（感知机）假设输入空间（特征空间）是 ${\cal X}\subseteq\mathbb{R}^n$，输出空间是 ${\cal Y}=\{+1,-1\}$。输入 $x\in{\cal X}$ 表示实例的特征向量，对应于输入空间（特征空间）的点；输出 $y\in{\cal Y}$ 表示实例的类别。由输入空间到输出空间的如下函数：
$$f(x)=\text{sign}(w\cdot x+b)$$
称为感知机。其中，$w$ 和 $b$ 为感知机模型参数，$w\in\mathbb{R}^n$ 叫做权值（weight）或权值向量（weight vector），$b\in\mathbb{R}$ 叫做偏置（bias），$w\cdot x$ 表示 $w$ 和 $x$ 的内积。sign 是符号函数，即：
$$\text{sign}(x)=\begin{cases}+1, & x\geq0 \\ -1, & x<0\end{cases}$$**

感知机是一种线性分类模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合：$\{f|f(x)=w\cdot x+b\}$。

感知机的几何解释：对于线性方程
$$w\cdot x+b=0$$
对应于特征空间 $\mathbb{R}^n$ 中的一个超平面 $S$，称为分离超平面（separating hyperplane），其中 $w$ 是超平面的法向量，$b$ 是超平面的截距。这个超平面将特征空间划分为两个部分。

## 2.2 感知机学习策略

### 2.2.1 数据集的线性可分性

**定义 2.2 （数据集的线性可分性）给定一个数据集
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
其中，$x_i\in{\cal X}=\mathbb{R}^n$，$y_i\in{\cal Y}=\{+1,-1\}$，$i=1,2,\cdots,N$，如果存在某个超平面 $S$
$$w\cdot x+b=0$$
能够将数据集的正负实例完全正确的划分到超平面的两侧，则称数据集 $T$ 是线性可分数据集（linearly separable data set）；否则，则称数据集 $T$ 线性不可分。**

### 2.2.2 感知机学习策略

为了找出超平面，需要一个学习策略，即定义（经验）损失函数并将损失函数极小化。

损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 $w$、$b$ 的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面 $S$ 的总距离，这是感知机所采用的。

首先，输入空间 $\mathbb{R}^n$ 中任一点 $x_0$ 到超平面 $S$ 的距离为：
$$\frac{1}{||w||}|w\cdot x+b|$$
这里，$||w||$ 是 $w$ 的 $L_2$ 范数。

其次，注意到对于误分类点数据 $(x_i,y_i)$，下式成立：
$$-y_i(w\cdot x_i+b)>0$$
因此，误分类点到超平面的总距离为：
$$-\frac{1}{||w||}\sum_{x_i\in M}y_i(w\cdot x_i+b)$$
其中，$M$ 是误分类点集合。不考虑 $\frac{1}{||w||}$，就得到感知机学习的损失函数。

给定训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
感知机 $\text{sign}(w\cdot x+b)$ 学习的损失函数定义为：
$$L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$

感知机学习的策略是在假设空间中选取使上面的损失函数最小的模型参数 $w$，$b$，即感知机模型。

## 2.3 感知机学习算法

为最优化上节的损失函数，采取随机梯度下降法（stochastic gradient descent）。

### 2.3.1 感知机学习算法的原始形式

感知机学习算法是对以下最优化问题的算法。给定训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
其中 $x_i\in{\cal X}=\mathbb{R}^n$，$y_i\in{\cal Y}=\{+1,-1\}$，$i=1,2,\cdots,N$，求参数 $w$，$b$，使其为以下损失函数极小化的一个解：
$$\min_{w,b} L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$
其中，$M$ 是误分类点集合。

随机梯度下降法步骤如下：首先，任意选取一个超平面 $w_0$，$b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程不是一次使 $M$ 中所有误分类法的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

假设误分类点集合 $M$ 是固定的，那么损失函数 $L(w,b)$ 的梯度为：
$$\nabla_wL(w,b)=-\sum_{x_i\in M}y_ix_i$$
$$\nabla_bL(w,b)=-\sum_{x_i\in M}y_i$$

随机选取一个误分类点 $(x_i,y_i)$ 对 $w$，$b$ 进行更新：
$$w\longleftarrow w+\eta y_ix_i$$
$$b\longleftarrow b+\eta y_i$$
式中 $\eta\ (0<\eta\leq 1)$ 是步长，也称学习率（learning rate）。

**算法 2.1 （感知机学习算法的原始形式)  
输入：训练数据集
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
其中
$$x_i\in{\cal X}=\mathbb{R}^n$，$y_i\in{\cal Y}=\{+1,-1\},i=1,2,\cdots,N$$
学习率
$$\eta\ (0<\eta\leq 1)$$
输出：$w$，$b$；
感知机模型
$$f(x)=\text{sign}(w\cdot x+b)$$
(1). 选取初值 $w_0$，$b_0$  
(2). 在训练集中选取数据 $(x_i,y_i)$  
(3). 如果 $y_i(w\cdot x_i+b)\leq 0$
$$w\longleftarrow w+\eta y_ix_i$$
$$b\longleftarrow b+\eta y_i$$
(4). 转至 (2)，直至训练集中没有误分类点**

### 2.3.2 算法的收敛性

为便于推导，将偏置并入权重向量，记为 $\hat{w}=(w^\text{T},b)^\text{T}$，同样也将输入向量加以扩充，加进常数 1，记作 $\hat{x}=(x^\text{T},1)^\text{T}$。这样，$\hat{x}\in\mathbb{R}^{n+1}$，$\hat{w}\in\mathbb{R}^{n+1}$，显然 $\hat{w}\cdot\hat{x}=w\cdot x+b$。

***
