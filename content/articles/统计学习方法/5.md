Title: 统计学习方法 第五章 决策树
Category: 读书笔记
Date: 2018-11-07 16:52:41
Modified: 2018-11-07 19:42:34
Tags: 统计学习, 机器学习

决策树（decision tree）是一种基本的分类与回归方法。其主要优点是模型具有可读性，分类速度快。决策树学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。

## 5.1 决策树模型与学习

### 5.1.1 决策树模型

**
定义 5.1 （决策树）分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）和叶节点（leaf node）。内部节点表示一个特征或属性，叶节点表示一个类。
**

### 5.1.2 决策树学习

假设给定训练数据集
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
其中输入实例（向量）为
$$x_i=\left(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)},\right)$$
$n$ 为特征个数。类标记为
$$y_i\in\{c_1,c_2,\cdots,c_K\}$$
其中 $i\in\{1,2,\cdots,N\}$，$N$ 为样本容量。

学习的目的是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练集不相矛盾的决策树可能有多个也可能一个也没有，我们需要的是一个与训练集矛盾较小的决策树，同时具有很好的泛化能力。

决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。

当损失函数确定以后，学习问题变成在损失函数意义下选择最有决策树的问题，因为从所有可能的决策树中选取最优决策树是 NP 完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal）的。

## 5.2 特征选择

### 5.2.1 特征选择的问题

特征选择在于选取对训练数据具有分类能力的特征。这样就可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。通常特征选取的准则是信息增益或信息增益比。

### 5.2.2 信息增益

为了便于说明，首先给出熵与条件熵的定义。

熵（entropy）是表示随机变量不确定性的度量。

设 $X$ 是一个离散随机变量，其概率分布为
$$P(X=x_i)=p_i,i=1,2,\cdots,n$$
则，随机变量 $X$ 的熵定义为
$$H(X)=-\sum_{i=1}^np_i\log p_i$$
若 $p_i=0$，定义 $0\log0=0$。通常，上式中对数的底为 2 或自然对数 e，这是熵的单位分别为比特（bit）或纳特（nat）。

设有随机变量 $(X,Y)$，其联合概率分布为
$$P(X=x_i,Y=y_j)=p_{ij},i=1,2,\cdots,n;j=1,2,\cdots,m$$
条件熵（conditional entropy） $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望
$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$
这里 $p_i=P(X=x_i),i=1,2,\cdots,n$

当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。

信息增益（information gain）表示得知特征 $X$ 的信息而使类 $Y$ 的信息的不确定度减少的程度。

**
定义 5.2 （信息增益）特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A）$只差
$$g(D,A)=H(D)-H(D|A)$$
**

一般的，熵与条件熵只差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类和特征的互信息。

根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，选择信息增益最大的特征。
