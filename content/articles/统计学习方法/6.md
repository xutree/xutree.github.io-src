Title: 统计学习方法 第六章 逻辑回归与最大熵模型
Category: 读书笔记
Date: 2018-11-09 16:17:17
Modified: 2018-11-09 17:33:29
Tags: 统计学习, 机器学习

逻辑回归（logistic regression）是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）。

逻辑回归模型和最大熵模型都属于对数线性模型。

## 6.1 逻辑回归模型

### 6.1.1 逻辑分布

**
定义 6.1（逻辑分布）设 $X$ 是连续随机变量，$X$ 服从逻辑分布是指 $X$ 具有下列分布函数和密度函数：
$$\begin{eqnarray}
F(x) &=& P(X\leq x)=\frac{1}{1+\text{e}^{-(x-\mu)/\gamma}} \\
f(x) &=& F'(x)=\frac{\text{e}^{-(x-\mu)/\gamma}}{\gamma(1+\text{e}^{-(x-\mu)/\gamma})^2}
\end{eqnarray}$$
式中，$\mu$ 为位置参数，$\gamma>0$ 为形状参数。
**

下图中绘制了对于不用 $\gamma$ 逻辑分布函数和概率密度函数。分布函数是一条 S 形曲线（sigmoid curve）。该曲线以点 $\left(\mu,\frac{1}{2}\right)$ 为中心对称，即满足
$$F(-x+\mu)+F(x+\mu)=1$$

![逻辑分布]({filename}/images/statistical_learning_6.1.png)

曲线在中心附近增长速度较快，在两端增长速度较慢。

### 6.1.2 二项 逻辑回归模型

二项逻辑回归模型（binomial logistic regression model）是一种分类模型，由条件概率 $P(Y|X)$ 表示，形式为参数化的逻辑分布。这里，随机变量 $X$ 取值为实数，随机变量 $Y$ 取值为 1 或 0，我们通过监督学习的方法来估计模型参数。

**
定义 6.2 （逻辑回归模型）二项逻辑回归模型是如下的条件概率分布：
$$\begin{eqnarray}
P(Y=1|x) &=& \frac{\exp(w\cdot x+b)}{1+\exp(w\cdot x+b)} \\
P(Y=0|x) &=& \frac{1}{1+\exp(w\cdot x+b)}
\end{eqnarray}$$
这里，$x\in\mathbb{R}^n$ 是输入，$Y\in\{0,1\}$ 是输出，$w\in\mathbb{R}^n$ 和 $b\in\mathbb{R}$ 是参数（分别称为权重向量和偏置），$w\cdot x$ 是内积。
**

对于给定的输入实例 $x$，按照定义 6.2 可以求得 $P(Y=1|x)$ 和 $P(Y=0|x)$。逻辑回归比较两个概率值的大小，将实例 $x$ 分到概率值较大的那一类。

有时为了方便，将权重向量和输入向量加以扩充，即
$$w=\left(w^{(1)},w^{(2)},\cdots,w^{(n)},1\right)^\text{T}\\
x=\left(x^{(1)},x^{(2)},\cdots,x^{(n)},1\right)^\text{T}$$
这时，逻辑回归模型如下：
$$\begin{eqnarray}
P(Y=1|x) &=& \frac{\exp(w\cdot x)}{1+\exp(w\cdot x)} \\
P(Y=0|x) &=& \frac{1}{1+\exp(w\cdot x)}
\end{eqnarray}$$

一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值，该事件的对数几率（log odds）或 logit 函数是
$$\text{logit}(p)=\log\frac{p}{1-p}$$

对逻辑回归而言，由扩充后的回归模型得
$$\text{logit}\left(P(Y=1|x)\right)=\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x$$
也就是说，输出 $Y=1$ 的对数几率是输入 $x$ 的线性函数，或者说，输出 $Y=1$ 的对数几率是由输入 $x$ 的线性函数表示的模型，即逻辑回归模型。

换一个角度看，考虑对输入 $x$ 进行分类的线性函数 $w\cdot x$（扩充后的），其值域为实数域。通过逻辑回归模型定义式可以将线性函数 $w\cdot x$ 转换为概率
$$P(Y=1|x)=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}$$
上面的分布实际上是逻辑分布。这是，线性函数的值越接近 $+\infty$，概率值就越接近 1；线性函数的值越接近 $-\infty$，概率在就越接近 0。这样的模型就是逻辑回归模型。

### 6.1.3 模型参数估计

逻辑回归模型学习时，对于给定的训练数据集
$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
其中 $x_i\in\mathbb{R}^n$，$y_i\in\{0,1\}$，可以应用极大似然估计法估计模型参数，从而得到逻辑回归模型。

设
$$P(Y=1|x) = \pi(x) \\
P(Y=0|x) = 1-\pi(x)$$
似然函数为
$$\prod_{i=1}^N\left[\pi(x_i)\right]^{y_i}\left[1-\pi(x_i)\right]^{1-y_i}$$
对数似然函数为
$$\begin{eqnarray}
L(w) &=& \sum_{i=1}^N\left[y_i\log\pi(x_i)+(1-y_i)\log(1-\pi(x_i))\right] \\
&=& \sum_{i=1}^N\left[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))\right] \\
&=& \sum_{i=1}^N[y_i(w\cdot x_i)-\log(1+\exp(w\cdot x_i))]
\end{eqnarray}$$
对 $L(w)$ 求极大值，得到 $w$ 的估计值。

这样，问题就变成以对数似然函数为目标函数的最优化问题。逻辑回归学习中通常采用的方法是梯度下降法和拟牛顿法。

假设 $w$ 的极大似然估计值是 $\hat{w}$，那么学到的逻辑回归模型为
$$P(Y=1|x)=\frac{\exp(\hat{w}\cdot x)}{1+\exp(\hat{w}\cdot x)} \\
P(Y=0|x)=\frac{1}{1+\exp(\hat{w}\cdot x)}$$

### 6.1.4 多项逻辑回归

可以将二项回归模型推广到多项逻辑回归模型（multi-nominal logistic regression model），用于多类分类。

假设离散型随机变量 $Y$ 的取值集合是 $\{1,2,\cdots,K\}$，那么多项逻辑回归模型是
$$P(Y=k|x)=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)},k=1,2,\cdots,K-1 \\
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}$$
这里，$x\in\mathbb{R}^{n+1}$，$w_k\in\mathbb{R}^{n+1}$
二项逻辑回归的参数估计法也可以推广到多项逻辑回归。
