Title: 统计学习方法 第一章 统计学习方法概论
Category: 读书笔记
Date: 2018-11-04 12:54:21
Modified: 2018-11-04 14:11:42
Tags: 统计学习, 机器学习

## 统计学习

### 特点

统计学习（statistical learning）是关于计算机基于**数据**构建概率统计模型并运用模型对数据进行**预测与分析**的一门学科。

赫尔伯特·西蒙（Herbert A.Simon）对“学习”给出如下定义：如果一个系统能够通过执行某个过程改变它的性能，这就是学习。

### 对象

统计学习的对象是数据。

统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。

### 目的

统计学习用于对数据进行预测和分析，特别是对未知新数据进行预测与分析。

### 方法

- 监督学习（supervised learning）
- 非监督学习（unsupervised learning）
- 半监督学习（semi-supervised learning）
- 强化学习（reinforcement learning）

### 统计学习方法三要素

- **模型（model）**：即假设空间（hypothesis space），假设空间是一个集合，这个集合包含要学习的模型
- **策略（strategy）**：模型选择的准则
- **算法（algorithm）**：模型学习的算法

## 监督学习

监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。

### 基本概念

- **输入空间（input space）**：输入的所有可能取值的集合，表示为 ${\cal X}$
- **输出空间（output space）**：输出的所有可能取值的集合，表示为 ${\cal Y}$
- **实例（instance）**：每个具体的输入，通常由特征向量（feature vector）表示
- **特征空间（feature space）**：所有特征向量存在的空间，每一维度对应一个特征
- **训练数据（training data）**：由输入（或特征向量）与输出对组成
- **联合概率分布（joint probability distribution）**：监督学习假设输入与输出的随机变量 $X$ 和 $Y$ 遵循联合概率分布 $P(X,Y)$
- **假设空间（hypothesis space）**：模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。假设空间的确定意味着学习范围的确定

**注1**：输入和输出空间可以使有限集也可以是整个欧式空间；输入与输出空间可以使用一个空间，也可以是不同的空间；通常输出空间远小于输入空间。

**注2**：有时假设输入空间与特征空间为相同的空间，对它们不予区分；有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。模型实际上都是定义在特征空间上的。

**注3**：在学习过程中，假设联合概率分布存在，但对于学习系统来说，联合概率分布的具体定义是未知的。训练数据与测试数据被看作是依联合概率分布 $P(X,Y)$ 独立同分布（independent and identically distribution）产生的。统计学习假设数据存在一定的统计规律，$X$ 和 $Y$ 遵循联合概率分布 $P(X,Y)$ 就是监督学习关于数据的基本假设。

在监督学习过程中，将输入和输出看做是定义在输入（特征）空间与输出空间上的随机变量的取值。输入、输出变量用大写字母表示，输入、输出变量所取的值用小写字母表示。

输入实例 $x$ 的特征向量记作：$x=\left(x^{(1)},x^{(2)},\dots,x^{(n)}\right)^\text{T}$，$x^{(i)}$ 表示 $x$ 的第 $i$ 个特征。

多个输入变量的第 $i$ 个记作：$x_i=\left(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)}\right)^\text{T}$

训练集：$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$

监督学习的模型可以是概率模型或非概率模型，由条件概率分布 $P(Y|X)$ 或决策函数（decision function）$Y=f(X)$ 表示。对具体的输入进行相应的输出预测时，写作 $P(y|x)$ 或 $y=f(x)$。

## 统计学习三要素

### 模型

统计学习首要考虑的问题是学习什么样的模型。模型的假设空间包含所有可能的条件概率分布或决策函数。假设空间中的模型一般有无穷多个。

假设空间用 $\cal{F}$ 表示。假设空间可以定义为决策函数或条件概率分布的集合：
$${\cal F}=\{f|Y=f_\theta(X),\theta \in \mathbb R^n\}\ \ 或\ \ {\cal F}=\{P|P_\theta(Y|X),\theta \in \mathbb R^n\}$$

参数向量 $\theta$ 取值于 $n$ 维欧式空间 $\mathbb R^n$，称为参数空间（parameter space）。

### 策略

有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。

#### 损失函数和风险函数

损失函数（loss function）或代价函数（cost function）用来度量预测错误的程度。损失函数是 $f(X)$ 和 $Y$ 的非负实值函数，记作 $L(Y,f(X))$。

统计学习常用的损失函数有以下几种：

1. 0-1 损失函数（0-1 loss function）
$$L(Y,f(X)=\begin{cases}
1, & Y\neq f(X) \\
0, & Y=f(X)
\end{cases}$$

2. 平方损失函数（quadratic loss function）
$$L(Y,f(X))=(Y-f(X))^2$$

3. 绝对损失函数（absolute loss function）
$$L(Y,f(X))=|Y-f(X)|$$

4. 对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）
$$L(Y,P(Y|X)=-\log P(Y|X)$$

风险函数（risk function）或期望损失（expected loss）：理论上模型 $f(X)$ 关于联合分布 $P(X,Y)$ 评价意义下的损失，记作 $R_\text{exp}$。
$$R_\text{exp}(f)=\text{E}_P[L(Y,f(X))]=\int_{{\cal X}\times{\cal Y}}L(y,f(x))P(x,y)dxdy$$

经验风险（empirical risk）或经验损失（empirical loss）：模型 $f(X)$ 关于训数据集的平均损失，记作 $R_\text{emp}$。
$$R_\text{emp}(f)=\frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)$$

#### 经验风险最小化

经验风险最小化（empirical risk minimization，ERM）策略认为：经验风险最小的模型是最优的模型。即求解下面的最优化问题：
$$ \min_{f\in {\cal F}} \frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)$$
其中 ${\cal F}$ 是假设空间。

当样本容量足够大时，经验风险最小化能保证有很好的学习效果。例如，极大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合（over-fitting）现象”。

#### 结构风险最小化

结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term），定义如下：
$$R_\text{srm}=\frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)+\lambda J(f)$$
其中 $J(f)$ 为模型的复杂度，是定义在假设空间 ${\cal F}$ 上的泛函。模型 $f$ 越复杂，$J(f)$ 越大。也就是说，复杂度表示了对复杂模型的惩罚，$\lambda\geq 0$ 是系数，用以权衡经验风险和模型复杂度。

结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。结构化风险最小化策略认为：结构风险最小的模型是最优的模型。即求解下面的最优化问题：
$$ \min_{f\in {\cal F}} \frac{1}{N}\sum_{i=1}^NL\left(y_i,f(x_i)\right)+\lambda J(f)$$

贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation，MAP）就是结构化风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

### 算法

算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

如果最优化问题有显式的解析解，这个最优化问题就比较简单，但通常解析解不存在，这就需要用数值方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。
